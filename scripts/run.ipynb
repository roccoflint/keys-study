{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for en with method: normal\n",
      "../models/cc.en.300.bin\n",
      "Finished loading model for en with method: normal\n",
      "Loading model for es with method: normal\n",
      "../models/cc.es.300.bin\n",
      "Finished loading model for es with method: normal\n",
      "Loading model for de with method: normal\n",
      "../models/cc.de.300.bin\n",
      "Finished loading model for de with method: normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import shutil\n",
    "\n",
    "marker = \"###################################\"\n",
    "\n",
    "# Combined language-specific configurations\n",
    "languages = {\n",
    "    'en': {\n",
    "        'full_name': 'English', \n",
    "        'model_path': '../models/cc.en.300.bin',\n",
    "        'exclude_words': [\"man\", \"woman\", \"Phil\", \"marv\", \"ole\", \"owld\", \"utd\"],\n",
    "        # \"particuler\", \"legendry\" -- archaic spellings\n",
    "        'genders': ['man', 'woman'],\n",
    "        'determiners': ['the'],\n",
    "        'personhood_word':'person',\n",
    "        'depersonalized_genders':['masculinity', 'femininity'],\n",
    "    },\n",
    "    'es': {\n",
    "        'full_name': 'Spanish', \n",
    "        'model_path': '../models/cc.es.300.bin',\n",
    "        'exclude_words': [\"hombre\", \"mujer\"],\n",
    "        'genders': ['hombre', 'mujer'],\n",
    "        'determiners': ['el', 'la'],\n",
    "        'personhood_word':'persona',\n",
    "        'depersonalized_genders':['masculinidad', 'femininidad'],\n",
    "    },\n",
    "    'de': {\n",
    "        'full_name': 'German', \n",
    "        'model_path': '../models/cc.de.300.bin',\n",
    "        'exclude_words': [\"Mann\", \"Frau\", \"mfG\", \"ein\"],\n",
    "        'genders': ['Mann', 'Frau'],\n",
    "        'determiners': ['der', 'die', 'das'],\n",
    "        'personhood_word':'Individuum',\n",
    "        'depersonalized_genders':['MÃ¤nnlichkeit', 'Weiblichkeit'],\n",
    "    }\n",
    "}\n",
    "\n",
    "columns = {\n",
    "    'masculine':'masculine_score',\n",
    "    'feminine':'feminine_score',\n",
    "}\n",
    "\n",
    "# Auto-generating parquet_paths based on languages\n",
    "parquet_paths = {lang: f\"../materials/adjectives/{lang}_adjectives.parquet\" for lang in languages.keys()}\n",
    "\n",
    "targets = ['masculine', 'feminine']\n",
    "\n",
    "nouns_df = pd.read_csv('../materials/nouns.csv')\n",
    "\n",
    "def load_model(language, method=\"normal\"):\n",
    "    \"\"\"Loads the specified language model into memory.\n",
    "\n",
    "    This function loads a model based on the language and method provided. It prints the model's path\n",
    "    and a message upon successful loading. The method can be either 'normal' or 'facebook', which\n",
    "    affects how the model is loaded.\n",
    "\n",
    "    Args:\n",
    "        language (str): The two-letter code of the language for which to load the model.\n",
    "        method (str, optional): The method of loading the model. Defaults to 'normal'. Options are:\n",
    "            - 'normal': Uses the standard loading mechanism.\n",
    "            - 'facebook': Uses a Facebook-specific loading mechanism.\n",
    "\n",
    "    Returns:\n",
    "        The loaded model.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the language code is not found in the languages dictionary.\n",
    "        FileNotFoundError: If the model file does not exist at the specified path.\n",
    "\n",
    "    Prints:\n",
    "        The path of the loaded model and a confirmation message.\n",
    "    \"\"\"\n",
    "    print(f'Loading model for {language} with method: {method}')\n",
    "    model_path = languages[language]['model_path']\n",
    "    print(model_path)\n",
    "    if method == 'normal': \n",
    "        model = fasttext.load_model(model_path)\n",
    "    else:\n",
    "        model = load_facebook_model(model_path)\n",
    "    print(f'Finished loading model for {language} with method: {method}')\n",
    "    return model\n",
    "\n",
    "models = {lang: load_model(lang, method='normal') for lang in languages.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base functions\n",
    "def cossim(vec1, vec2):\n",
    "    \"\"\"Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    This function computes the cosine similarity between two vectors `vec1` and `vec2`.\n",
    "    Cosine similarity is a measure of similarity between two non-zero vectors of an inner product\n",
    "    space that measures the cosine of the angle between them.\n",
    "\n",
    "    Args:\n",
    "        vec1 (list of float): First vector.\n",
    "        vec2 (list of float): Second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity between `vec1` and `vec2`, ranging from -1 meaning exactly opposite,\n",
    "               to 1 meaning exactly the same, with 0 indicating orthogonality (decorrelation), and\n",
    "               in-between values indicating intermediate similarity or dissimilarity.\n",
    "\n",
    "    \"\"\"\n",
    "    dot_product = sum(a*b for a, b in zip(vec1, vec2))\n",
    "    magnitude1 = math.sqrt(sum([val**2 for val in vec1]))\n",
    "    magnitude2 = math.sqrt(sum([val**2 for val in vec2]))\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def get(model, word):\n",
    "    \"\"\"Retrieves the word embedding for a given word from a model.\n",
    "\n",
    "    This function returns the word embedding for `word` from the specified `model`.\n",
    "    Word embeddings are a type of word representation that allows words to be represented\n",
    "    as vectors in a continuous vector space.\n",
    "\n",
    "    Args:\n",
    "        model (Model): The model from which to retrieve the word embedding.\n",
    "        word (str): The word for which to retrieve the embedding.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The embedding of `word` as a dense vector.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If `word` is not in the model's vocabulary.\n",
    "\n",
    "    \"\"\"\n",
    "    return model.get_word_vector(word)\n",
    "\n",
    "def load_dataframe(file_path):\n",
    "    \"\"\"Loads a DataFrame from a file at `file_path`.\n",
    "\n",
    "    This function supports loading from CSV and Parquet files. It checks the file extension\n",
    "    and loads the DataFrame accordingly. \n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to load.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the data from `file_path`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file format is not supported.\n",
    "\n",
    "    \"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extension == '.csv':\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_extension == '.parquet':\n",
    "        return pd.read_parquet(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def fetch_html_content(url):\n",
    "    \"\"\"Fetches the HTML content from a given URL.\n",
    "\n",
    "    This function sends a GET request to the specified `url` and returns its HTML content\n",
    "    as a string. It raises an HTTPError if the request encounters an error.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL from which to fetch the HTML content.\n",
    "\n",
    "    Returns:\n",
    "        str: The HTML content of the webpage.\n",
    "\n",
    "    Raises:\n",
    "        HTTPError: If the request fails due to client or server HTTP errors.\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "# Wiktionary crawling\n",
    "def parse_adjectives(soup):\n",
    "    \"\"\"Extracts adjectives from a BeautifulSoup object containing Wiktionary page content.\n",
    "\n",
    "    This function parses a BeautifulSoup object for adjectives listed on a Wiktionary page. It filters out\n",
    "    entries containing digits, spaces, hyphens, plus signs, ampersands, apostrophes, periods, and parentheses.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the Wiktionary page.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of adjectives extracted from the page.\n",
    "    \"\"\"\n",
    "    mw_category_groups = soup.find_all(class_=\"mw-category-group\")\n",
    "    adjectives = []\n",
    "    for group in mw_category_groups:\n",
    "        li_tags = group.find_all('li')\n",
    "        for li in li_tags:\n",
    "            adjective = li.get_text()\n",
    "            if not any(char.isdigit() for char in adjective) and \" \" not in adjective and \"-\" not in adjective and \"+\" not in adjective and \"&\" not in adjective and \"'\" not in adjective and \".\" not in adjective and \"(\" not in adjective:\n",
    "                adjectives.append(adjective)\n",
    "    return adjectives\n",
    "\n",
    "def find_next_page_url(soup):\n",
    "    \"\"\"Finds and returns the URL of the next page in a Wiktionary category listing.\n",
    "\n",
    "    This function searches for a 'next page' link in a BeautifulSoup object and constructs the full URL\n",
    "    to the next page of a Wiktionary category listing if such a link exists.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the current page.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The URL of the next page if found, otherwise None.\n",
    "    \"\"\"\n",
    "    next_page_link = soup.find(\"a\", string=\"next page\")\n",
    "    return 'https://en.wiktionary.org' + next_page_link.get('href') if next_page_link else None\n",
    "\n",
    "def extract_adjectives(language, url, max_pages=None):\n",
    "    \"\"\"Extracts adjectives for a given language from Wiktionary starting from a specific URL.\n",
    "\n",
    "    This function crawls Wiktionary pages to collect adjectives for the specified language. It iterates through pages,\n",
    "    parsing and collecting adjectives until a maximum number of pages is reached or no further pages are found.\n",
    "\n",
    "    Args:\n",
    "        language (str): The language for which to extract adjectives.\n",
    "        url (str): The starting URL for crawling.\n",
    "        max_pages (int, optional): The maximum number of pages to crawl. If None, crawls without limit.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of collected adjectives.\n",
    "    \"\"\"\n",
    "    all_adjectives = []\n",
    "    page_count = 0\n",
    "    while url and (max_pages is None or page_count < max_pages):\n",
    "        html_content = fetch_html_content(url)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        adjectives = parse_adjectives(soup)\n",
    "        all_adjectives.extend(adjectives)\n",
    "        url = find_next_page_url(soup)\n",
    "        page_count += 1\n",
    "    return all_adjectives\n",
    "\n",
    "def save_adjectives_to_parquet(adjectives, language_code, file_path):\n",
    "    \"\"\"Saves a list of adjectives to a Parquet file.\n",
    "\n",
    "    Takes a list of adjectives and their corresponding language code, then saves this information into a Parquet file. \n",
    "    Parquet is chosen for its efficiency in both storage and speed when handling data operations within pandas DataFrames.\n",
    "\n",
    "    Args:\n",
    "        adjectives (list of str): The list of adjectives to be saved.\n",
    "        language_code (str): The ISO 639-1 language code representing the language of the adjectives.\n",
    "        file_path (str): The path to where the Parquet file will be saved, including the file name and its extension.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file_path directory does not exist.\n",
    "        ValueError: If `adjectives` or `language_code` are empty.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(adjectives, columns=['Adjective'])\n",
    "    df['Language'] = language_code\n",
    "    df.to_parquet(file_path, index=False)\n",
    "\n",
    "def calculate_adjective_similarities(language_code, method=cossim):\n",
    "    \"\"\"Calculates and adds gender-related similarity scores to adjectives for a given language.\n",
    "\n",
    "    This function calculates gender-related similarity scores for each adjective in a specified language using a word embedding model. It loads adjectives from a Parquet file, computes similarities with predefined gender-related target words using a specified similarity function (defaulting to cosine similarity), and updates the DataFrame with these similarity scores for various categories, including masculine, feminine, exclusive masculine, exclusive feminine, depersonalized masculine, and depersonalized feminine similarities. Finally, it saves the updated DataFrame back to a Parquet file.\n",
    "\n",
    "    Args:\n",
    "        language_code (str): The ISO 639-1 language code for the target language.\n",
    "        method (function, optional): The function to use for calculating similarity between vectors. Defaults to cosine similarity.\n",
    "\n",
    "    Note:\n",
    "        This function requires a predefined mapping in `parquet_paths` for loading the DataFrame, a `models` dictionary with loaded word embedding models for each language, and `languages` dictionary containing target words for each gender category in the specified language.\n",
    "    \"\"\"\n",
    "    # Load the Parquet file into a DataFrame\n",
    "    parquet_file_path = parquet_paths[language_code]\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "    # Load the word embedding model\n",
    "    language_data = languages[language_code]\n",
    "    model = models[language_code]\n",
    "\n",
    "    # Initialize columns for similarities\n",
    "    df['masculine_similarity'] = 0.0\n",
    "    df['feminine_similarity'] = 0.0\n",
    "    df['exclusive_masculine_similarity'] = 0.0\n",
    "    df['exclusive_feminine_similarity'] = 0.0\n",
    "    df['depersonalized_masculine_similarity'] = 0.0\n",
    "    df['depersonalized_feminine_similarity'] = 0.0\n",
    "\n",
    "    # Get target word embeddings\n",
    "    masculine_target = get(model, language_data['genders'][0])\n",
    "    feminine_target = get(model, language_data['genders'][1])\n",
    "    neuter_target = get(model, language_data['personhood_word'])\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    for index, row in df.iterrows():\n",
    "        word_vec = get(model, row['Adjective'])\n",
    "\n",
    "        # Regular similarities\n",
    "        df.at[index, 'masculine_similarity'] = method(word_vec, masculine_target)\n",
    "        df.at[index, 'feminine_similarity'] = method(word_vec, feminine_target)\n",
    "        df.at[index, 'neuter_similarity'] = method(word_vec, neuter_target)\n",
    "        df.at[index, 'depersonalized_masculine_similarity'] = method(word_vec, get(models[language_code], language_data['depersonalized_genders'][1]))\n",
    "        df.at[index, 'depersonalized_feminine_similarity'] = method(word_vec, get(models[language_code], language_data['depersonalized_genders'][0]))\n",
    "\n",
    "    # Save the updated DataFrame back to Parquet\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "def select_top_words(language_code, method, num_rows=1000, semantic_differential_vectors='gender1-gender2'):\n",
    "    \"\"\"Selects top adjectives based on gender-related scores from a dataset for a given language.\n",
    "\n",
    "    This function loads adjectives from a Parquet file and applies either a semantic differential method or \n",
    "    cosine similarity to score them based on gender-related dimensions. It then selects the top scoring adjectives \n",
    "    for masculine and feminine categories while excluding specific words. The selected adjectives are saved to \n",
    "    separate Parquet files for each gender category.\n",
    "\n",
    "    Args:\n",
    "        language_code (str): The ISO 639-1 language code for the target language.\n",
    "        method (str): The method to apply for scoring adjectives. Can be 'semantic_differential' or 'cosine_similarity'.\n",
    "        num_rows (int, optional): The number of top adjectives to select for each gender category. Defaults to 1000.\n",
    "        semantic_differential_vectors (str, optional): Specifies the vectors to use for the semantic differential \n",
    "            method. Defaults to 'gender1-gender2'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two pandas DataFrames, the first containing the selected masculine adjectives, and the second containing \n",
    "        the selected feminine adjectives.\n",
    "    \"\"\"\n",
    "    # Load the Parquet file into a DataFrame\n",
    "    parquet_file_path = parquet_paths[language_code]\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "    # Access language-specific data\n",
    "    language_data = languages[language_code]\n",
    "\n",
    "    # Implement the semantic differential method\n",
    "    if method == 'semantic_differential':\n",
    "        if semantic_differential_vectors == 'gender1-gender2':\n",
    "            df['masculine_score'] = df['masculine_similarity'] - df['feminine_similarity']\n",
    "            df['feminine_score'] = df['feminine_similarity'] - df['masculine_similarity']\n",
    "        elif semantic_differential_vectors == 'gender-person':\n",
    "            df['masculine_score'] = df['masculine_similarity'] - df['neuter_similarity']\n",
    "            df['feminine_score'] = df['feminine_similarity'] - df['neuter_similarity']\n",
    "        elif semantic_differential_vectors == 'gender-Gender':\n",
    "            df['masculine_score'] = df['masculine_similarity'] - df['depersonalized_feminine_similarity']\n",
    "            df['feminine_score'] = df['feminine_similarity'] - df['depersonalized_masculine_similarity']\n",
    "    elif method == 'cosine_similarity':\n",
    "        df['masculine_score'] = df['masculine_similarity']\n",
    "        df['feminine_score'] = df['feminine_similarity']\n",
    "\n",
    "    # Filter out excluded words\n",
    "    exclude_list = language_data['exclude_words']\n",
    "    df = df[~df['Adjective'].isin(exclude_list)]\n",
    "\n",
    "    # Initialize empty DataFrames for selected words\n",
    "    selected_masculine = pd.DataFrame(columns=df.columns)\n",
    "    selected_feminine = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # Select top adjectives for each gender\n",
    "    while len(selected_masculine) < num_rows or len(selected_feminine) < num_rows:\n",
    "        if len(selected_masculine) < num_rows:\n",
    "            top_masculine = df.sort_values(by='masculine_score', ascending=False).head(num_rows * 2)\n",
    "            top_masculine = top_masculine[~top_masculine['Adjective'].isin(selected_feminine['Adjective'])].head(num_rows - len(selected_masculine))\n",
    "            selected_masculine = pd.concat([selected_masculine, top_masculine])\n",
    "\n",
    "        if len(selected_feminine) < num_rows:\n",
    "            top_feminine = df.sort_values(by='feminine_score', ascending=False).head(num_rows * 2)\n",
    "            top_feminine = top_feminine[~top_feminine['Adjective'].isin(selected_masculine['Adjective'])].head(num_rows - len(selected_feminine))\n",
    "            selected_feminine = pd.concat([selected_feminine, top_feminine])\n",
    "\n",
    "    # Save the selected words to Parquet files\n",
    "    masculine_file_path = f'../materials/adjectives/{language_code}_masculine_adjectives.parquet'\n",
    "    feminine_file_path = f'../materials/adjectives/{language_code}_feminine_adjectives.parquet'\n",
    "    selected_masculine.to_parquet(masculine_file_path, index=False)\n",
    "    selected_feminine.to_parquet(feminine_file_path, index=False)\n",
    "\n",
    "    return selected_masculine, selected_feminine\n",
    "\n",
    "def duplicate_spanish_adjectives(df, association):\n",
    "    \"\"\"Generates alternate forms of Spanish adjectives based on gender and updates the DataFrame.\n",
    "\n",
    "    This function adds an 'Alternate Form' column to a DataFrame containing Spanish adjectives. It generates the \n",
    "    opposite gender form of each adjective based on its ending ('o' for masculine to 'a' for feminine and vice versa) \n",
    "    and updates the DataFrame with these alternate forms. Finally, it saves the updated DataFrame to a Parquet file, \n",
    "    differentiating the file name based on the association (masculine or feminine) of the input adjectives.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing Spanish adjectives.\n",
    "        association (str): The gender association ('masculine' or 'feminine') of the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The updated DataFrame including the 'Alternate Form' column with opposite gender endings.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the association parameter is not 'masculine' or 'feminine'.\n",
    "    \"\"\"\n",
    "    # Add 'Alternate Form' column based on gender association\n",
    "    df['Alternate Form'] = df['Adjective'].apply(lambda x: x[:-1] + 'a' if x.endswith('o') else x[:-1] + 'o')\n",
    "\n",
    "    # Save the updated DataFrame to a Parquet file\n",
    "    file_path = f'../materials/adjectives/es_{association}_adjectives.parquet'\n",
    "    df.to_parquet(file_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_similarity(model, words, target_words, ref_group_label, language, ref_association, target_group):\n",
    "    \"\"\"Calculates the cosine similarity between sets of word vectors and target vectors.\n",
    "\n",
    "    This function computes the cosine similarity for each pair consisting of a 'reference word' from the given 'words' \n",
    "    list and a 'target word' from the 'target_words' list. It uses a specified model to get the vector representations \n",
    "    of these words. The results, including language, reference group label, reference association, reference word, target \n",
    "    group, target word, and the calculated cosine similarity, are compiled into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        model: The word embedding model used to get vector representations.\n",
    "        words (list of str): The list of reference words to compare.\n",
    "        target_words (list of str): The list of target words to compare against the reference words.\n",
    "        ref_group_label (str): Label for the reference group (e.g., 'Adjectives').\n",
    "        language (str): The language of the words being compared.\n",
    "        ref_association (str): Association of the reference group (e.g., 'Positive', 'Negative').\n",
    "        target_group (str): Label for the target word group (e.g., 'Gender').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the language, reference group label, reference association, reference \n",
    "        word, target group, target word, and the cosine similarity for each comparison.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for word in words:\n",
    "        word_vec = get(model, word)\n",
    "        for target_word in target_words:\n",
    "            target_vec = get(model, target_word)\n",
    "            similarity = cossim(word_vec, target_vec)\n",
    "            results.append({\n",
    "                'LANGUAGE': language,\n",
    "                'REFERENCE GROUP': ref_group_label,\n",
    "                'REFERENCE ASSOCIATION': ref_association,\n",
    "                'REFERENCE WORD': word,\n",
    "                'TARGET GROUP': target_group,\n",
    "                'TARGET WORD': target_word,\n",
    "                'COSINE SIMILARITY': similarity\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def create_control_test_dataframe(lang_code, nouns_df, model):\n",
    "    \"\"\"Creates a control test DataFrame for a specified language.\n",
    "\n",
    "    This function generates a DataFrame for testing control groups in linguistic research, specifically focusing on nouns \n",
    "    and adjectives. It computes the cosine similarity of nouns and adjectives in the specified language to target words \n",
    "    defined in a global language configuration, accounting for grammatical gender and other associations.\n",
    "\n",
    "    Args:\n",
    "        lang_code (str): The ISO code for the target language.\n",
    "        nouns_df (pd.DataFrame): A DataFrame containing nouns and their associations/grammatical genders.\n",
    "        model: The word embedding model used to retrieve vector representations.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame with cosine similarity scores for nouns and adjectives across specified \n",
    "        target groups ('genders', 'determiners').\n",
    "    \"\"\"\n",
    "    control_test_data = []\n",
    "\n",
    "    # Process nouns\n",
    "    nouns_lang_df = nouns_df[nouns_df['LANGUAGE'] == lang_code]\n",
    "    for noun_gender in nouns_lang_df['ASSOCIATION/GRAMMATICAL GENDER'].unique():\n",
    "        nouns = nouns_lang_df[nouns_lang_df['ASSOCIATION/GRAMMATICAL GENDER'] == noun_gender]['WORD'].tolist()\n",
    "        for target_group in ['genders', 'determiners']:\n",
    "            target_words = languages[lang_code][target_group]\n",
    "            for target_word in target_words:\n",
    "                data = calculate_similarity(model, nouns, [target_word], 'nouns', lang_code, noun_gender, target_group)\n",
    "                control_test_data.append(data)\n",
    "\n",
    "    # Process adjectives for each gender association\n",
    "    for adj_association in ['masculine', 'feminine']:  # Assuming 'targets' is meant to be these associations\n",
    "        adjectives_df = pd.read_parquet(f'../materials/adjectives/{lang_code}_{adj_association}_adjectives.parquet')\n",
    "        adjectives = adjectives_df['Adjective'].tolist()\n",
    "        for target_group in ['genders', 'determiners']:\n",
    "            target_words = languages[lang_code][target_group]\n",
    "            for target_word in target_words:\n",
    "                data = calculate_similarity(model, adjectives, [target_word], 'adjectives', lang_code, adj_association, target_group)\n",
    "                control_test_data.append(data)\n",
    "\n",
    "            if lang_code == 'es' and 'Alternate Form' in adjectives_df.columns:\n",
    "                alternate_forms = adjectives_df['Alternate Form'].dropna().tolist()\n",
    "                for alternate_form in alternate_forms:\n",
    "                    data = calculate_similarity(model, [alternate_form], [target_word], 'adjectives', lang_code, adj_association, target_group)\n",
    "                    control_test_data.append(data)\n",
    "\n",
    "    combined_data = pd.concat(control_test_data, ignore_index=True)\n",
    "    return combined_data\n",
    "\n",
    "def create_experimental_test_dataframe(lang_code, nouns_df, model, use_groupby=False):\n",
    "    \"\"\"Creates an experimental test DataFrame for a specified language.\n",
    "\n",
    "    This function generates a DataFrame for conducting experimental linguistic tests, focusing on the relationship \n",
    "    between nouns and adjectives, including alternate forms of adjectives in languages like Spanish. It calculates \n",
    "    cosine similarities between nouns and adjectives, and optionally groups results to average similarities across \n",
    "    grammatical genders and adjective associations.\n",
    "\n",
    "    Args:\n",
    "        lang_code (str): The ISO code for the target language.\n",
    "        nouns_df (pd.DataFrame): A DataFrame containing nouns and their grammatical genders.\n",
    "        model: The word embedding model used to retrieve vector representations.\n",
    "        use_groupby (bool, optional): A flag to determine if the output should be grouped by grammatical gender \n",
    "            of nouns and gender association of adjectives. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with cosine similarity scores, including additional metadata such as the language, \n",
    "        grammatical gender of nouns, and gender association of adjectives.\n",
    "    \"\"\"\n",
    "    experimental_test_data = []\n",
    "\n",
    "    nouns_lang_df = nouns_df[nouns_df['LANGUAGE'] == lang_code]\n",
    "\n",
    "    for adj_association in ['masculine', 'feminine']:\n",
    "        adjectives_df = pd.read_parquet(f'../materials/adjectives/{lang_code}_{adj_association}_adjectives.parquet')\n",
    "        adjectives = adjectives_df['Adjective'].tolist()\n",
    "\n",
    "        if lang_code == 'es':\n",
    "            alternate_forms = adjectives_df['Alternate Form'].dropna().tolist()\n",
    "\n",
    "            for noun_gender in nouns_lang_df['ASSOCIATION/GRAMMATICAL GENDER'].unique():\n",
    "                nouns = nouns_lang_df[nouns_lang_df['ASSOCIATION/GRAMMATICAL GENDER'] == noun_gender]['WORD'].tolist()\n",
    "                for noun in nouns:\n",
    "                    for alternate_form in alternate_forms:\n",
    "                        similarity = cossim(model.get_word_vector(noun), model.get_word_vector(alternate_form))\n",
    "                        experimental_test_data.append({\n",
    "                            'LANGUAGE': lang_code,\n",
    "                            'GRAMMATICAL GENDER OF NOUN': noun_gender,\n",
    "                            'NOUN': noun,\n",
    "                            'ADJECTIVE': alternate_form,\n",
    "                            'COSINE SIMILARITY': similarity,\n",
    "                            'GENDER ASSOCIATION OF ADJECTIVE': adj_association\n",
    "                        })\n",
    "\n",
    "        for noun_gender in nouns_lang_df['ASSOCIATION/GRAMMATICAL GENDER'].unique():\n",
    "            nouns = nouns_lang_df[nouns_lang_df['ASSOCIATION/GRAMMATICAL GENDER'] == noun_gender]['WORD'].tolist()\n",
    "            for noun in nouns:\n",
    "                for adjective in adjectives:\n",
    "                    similarity = cossim(model.get_word_vector(noun), model.get_word_vector(adjective))\n",
    "                    experimental_test_data.append({\n",
    "                        'LANGUAGE': lang_code,\n",
    "                        'GRAMMATICAL GENDER OF NOUN': noun_gender,\n",
    "                        'NOUN': noun,\n",
    "                        'ADJECTIVE': adjective,\n",
    "                        'COSINE SIMILARITY': similarity,\n",
    "                        'GENDER ASSOCIATION OF ADJECTIVE': adj_association\n",
    "                    })\n",
    "\n",
    "    combined_data = pd.DataFrame(experimental_test_data)\n",
    "    \n",
    "    if use_groupby:\n",
    "        combined_data = combined_data.groupby(['LANGUAGE', 'GRAMMATICAL GENDER OF NOUN', 'NOUN', 'GENDER ASSOCIATION OF ADJECTIVE'])['COSINE SIMILARITY'].mean().reset_index()\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def find_adjective_definition(adjective):\n",
    "    \"\"\"Finds and returns the definition of an adjective using Wiktionary.\n",
    "\n",
    "    This function searches Wiktionary for the given adjective and extracts the first definition it finds. It's designed to \n",
    "    handle English words but can be adjusted for other languages by modifying the URL accordingly.\n",
    "\n",
    "    Args:\n",
    "        adjective (str): The adjective for which to find the definition.\n",
    "\n",
    "    Returns:\n",
    "        str: The first definition of the adjective if found; otherwise, a message indicating the definition was not found.\n",
    "    \"\"\"\n",
    "    url = f\"https://en.wiktionary.org/wiki/{adjective}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        definition_section = soup.find('span', {'id': 'Adjective'})\n",
    "        if definition_section:\n",
    "            definition_list = definition_section.find_next('ol')\n",
    "            if definition_list:\n",
    "                first_item = definition_list.find('li')\n",
    "                if first_item:\n",
    "                    definition = first_item.get_text(separator=' ', strip=True).split('.')[0]\n",
    "                    return definition\n",
    "    except requests.HTTPError as e:\n",
    "        return f\"Error retrieving page for {adjective}: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {adjective}: {e}\"\n",
    "\n",
    "    return \"Definition not found.\"\n",
    "\n",
    "def remove_adjective_duplicates(lang_code, columns):\n",
    "    \"\"\"\n",
    "    Removes duplicates between masculine and feminine adjective lists for a given language,\n",
    "    retaining only the form with the higher score. Specifically handles Spanish adjectives\n",
    "    by comparing masculine and feminine forms and removing the form with the lower score.\n",
    "    For non-Spanish or non-gender-inflected languages, it removes common adjectives\n",
    "    based on comparison scores.\n",
    "\n",
    "    Args:\n",
    "        lang_code (str): The ISO code for the language, used to load the appropriate CSV files.\n",
    "        columns (dict): A dictionary specifying the score columns to use for comparison,\n",
    "                        with keys 'masculine' and 'feminine'.\n",
    "\n",
    "    This function updates the CSV files by removing the lower-scored duplicates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load CSV files for masculine and feminine adjectives\n",
    "    masculine_csv = f'../materials/adjectives/{lang_code}_masculine_adjectives.csv'\n",
    "    feminine_csv = f'../materials/adjectives/{lang_code}_feminine_adjectives.csv'\n",
    "    df_masculine = pd.read_csv(masculine_csv)\n",
    "    df_feminine = pd.read_csv(feminine_csv)\n",
    "\n",
    "    # Handle Spanish adjectives with gender-specific endings\n",
    "    if lang_code == 'es':\n",
    "        for adj in df_masculine['Adjective']:\n",
    "            if adj.endswith('o'):  # Identifying masculine form\n",
    "                adj_root = adj[:-1]  # Removing gender-specific ending\n",
    "                feminine_form = adj_root + 'a'  # Constructing feminine form\n",
    "                # Check if the feminine form exists in the feminine adjectives DataFrame\n",
    "                if feminine_form in df_feminine['Adjective'].values:\n",
    "                    # Compare scores between masculine and feminine forms\n",
    "                    masculine_score = df_masculine.loc[df_masculine['Adjective'] == adj, columns['masculine']].values[0]\n",
    "                    feminine_score = df_feminine.loc[df_feminine['Adjective'] == feminine_form, columns['feminine']].values[0]\n",
    "                    # Remove the form with the lower score\n",
    "                    if masculine_score > feminine_score:\n",
    "                        df_feminine = df_feminine[df_feminine['Adjective'] != feminine_form]\n",
    "                    elif feminine_score > masculine_score:\n",
    "                        df_masculine = df_masculine[df_masculine['Adjective'] != adj]\n",
    "\n",
    "    # Handle duplicates in non-Spanish languages or non-gender inflected adjectives\n",
    "    common_adjectives = set(df_masculine['Adjective']).intersection(df_feminine['Adjective'])\n",
    "    for adj in common_adjectives:\n",
    "        # Compare scores for common adjectives\n",
    "        masculine_score = df_masculine.loc[df_masculine['Adjective'] == adj, columns['masculine']].values[0]\n",
    "        feminine_score = df_feminine.loc[df_feminine['Adjective'] == adj, columns['feminine']].values[0]\n",
    "        # Remove the form with the lower score\n",
    "        if masculine_score > feminine_score:\n",
    "            df_feminine = df_feminine[df_feminine['Adjective'] != adj]\n",
    "        elif feminine_score > masculine_score:\n",
    "            df_masculine = df_masculine[df_masculine['Adjective'] != adj]\n",
    "\n",
    "    # Save the updated CSV files back to their respective locations\n",
    "    df_masculine.to_csv(masculine_csv, index=False)\n",
    "    df_feminine.to_csv(feminine_csv, index=False)\n",
    "    print(f\"Updated CSV files for {lang_code}: removed duplicates with lower scores.\")\n",
    "\n",
    "def create_adjective_review_csv(parquet_file):\n",
    "    \"\"\"\n",
    "    Converts a parquet file containing adjectives into a CSV file with additional information\n",
    "    including definitions and similarity scores. The output is tailored based on the language\n",
    "    and gender association of the adjectives.\n",
    "\n",
    "    Args:\n",
    "        parquet_file (str): The path to the input parquet file containing adjectives.\n",
    "    \n",
    "    This function reads the specified parquet file, adds a 'Definition' column by looking up\n",
    "    each adjective's definition, sorts the data based on a gender-specific similarity score,\n",
    "    and saves the resulting DataFrame to a CSV file in a designated directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract language code and gender from the filename\n",
    "    file_name = os.path.basename(parquet_file)\n",
    "    language_code, gender = file_name.split('_')[:2]\n",
    "    print(f'Creating review sheet for language: {language_code}')\n",
    "\n",
    "    # Load the parquet file into a DataFrame\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Add a column with definitions for each adjective\n",
    "    df['Definition'] = df['Adjective'].apply(lambda x: find_adjective_definition(x))\n",
    "\n",
    "    # Identify the columns for similarity scores based on gender\n",
    "    similarity_score_col = f\"{gender.lower()}_similarity\"\n",
    "    score_col = f\"{gender.lower()}_score\"\n",
    "\n",
    "    # Sort the DataFrame based on the specified similarity score column\n",
    "    df_sorted = df.sort_values(similarity_score_col, ascending=False)\n",
    "\n",
    "    # Define the path where the CSV file will be saved\n",
    "    reviews_dir = '../materials/adjectives'\n",
    "    csv_file_path = os.path.join(reviews_dir, file_name.replace('.parquet', '.csv'))\n",
    "\n",
    "    # Save the sorted DataFrame to a CSV file\n",
    "    df_sorted.to_csv(csv_file_path, index=False)\n",
    "    print(f\"File saved as {csv_file_path}\")\n",
    "\n",
    "def find_minimum_length(languages, genders, unallowed_words):\n",
    "    \"\"\"\n",
    "    Finds the minimum length (number of entries) among various adjective lists across languages and genders,\n",
    "    excluding specified unallowed words.\n",
    "\n",
    "    This function iterates over a set of languages and genders, loading corresponding adjective lists from\n",
    "    parquet files. It filters out any unallowed words from these lists and determines the minimum length\n",
    "    among them.\n",
    "\n",
    "    Args:\n",
    "        languages (list): A list of language codes (str) to be processed.\n",
    "        genders (list): A list of genders (str) corresponding to the adjective lists to be processed.\n",
    "        unallowed_words (list): A list of words (str) that are to be excluded from the analysis.\n",
    "\n",
    "    Returns:\n",
    "        int: The minimum length (number of valid entries) found among the processed adjective lists.\n",
    "    \"\"\"\n",
    "    min_length = float('inf')  # Initialize with infinity\n",
    "\n",
    "    # Iterate over each language and gender to process corresponding adjective lists\n",
    "    for lang_code in languages:\n",
    "        for gender in genders:\n",
    "            # Construct the file path for the parquet file\n",
    "            parquet_file = f'../materials/adjectives/{lang_code}_{gender}_adjectives.parquet'\n",
    "            # Load the parquet file into a DataFrame\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "            # Exclude unallowed words from the DataFrame\n",
    "            df_filtered = df[~df['Adjective'].isin(unallowed_words)]\n",
    "            # Update min_length if the length of the current DataFrame is smaller\n",
    "            min_length = min(min_length, len(df_filtered))\n",
    "\n",
    "    return min_length\n",
    "\n",
    "def remove_unwanted_adjectives(csv_file, allowed_words, unallowed_words, markers, min_length, gender):\n",
    "    \"\"\"\n",
    "    Processes an adjective list CSV file by removing unallowed words, filtering out words with specific markers,\n",
    "    and trimming the list to a specified minimum length based on gender-specific scores.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing the adjective list.\n",
    "        allowed_words (set): A set of words that are explicitly allowed, even if they contain unwanted markers.\n",
    "        unallowed_words (set): A set of words that are explicitly disallowed and should be removed from the list.\n",
    "        markers (list): A list of substrings that, if found within an adjective's definition, mark the adjective as unwanted.\n",
    "        min_length (int): The number of top-scoring adjectives to retain in the list after processing.\n",
    "        gender (str): The gender association of the adjectives, used to determine the score column for sorting.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame after filtering and trimming operations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data from the specified CSV file into a DataFrame.\n",
    "    csv_df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract the language code from the filename, assuming it follows a specific naming convention.\n",
    "    lang_code = csv_file.split('/')[-1][:2]\n",
    "\n",
    "    # Ensure the 'Adjective' column is treated as strings for consistent processing.\n",
    "    csv_df['Adjective'] = csv_df['Adjective'].astype(str)\n",
    "\n",
    "    # Special preprocessing step for Spanish adjectives to handle gender variations.\n",
    "    if lang_code == 'es':\n",
    "        processed_unallowed = set()\n",
    "        for word in unallowed_words:\n",
    "            # For gendered words, add the root to the set of unallowed words.\n",
    "            if word.endswith('o') or word.endswith('a'):\n",
    "                processed_unallowed.add(word[:-1])\n",
    "            else:\n",
    "                processed_unallowed.add(word)\n",
    "        unallowed_words = processed_unallowed\n",
    "\n",
    "    # Filter out unallowed words by comparing roots for gendered words.\n",
    "    csv_df = csv_df[~csv_df['Adjective'].apply(lambda x: x[:-1] if (x.endswith('o') or x.endswith('a')) else x).isin(unallowed_words)]\n",
    "\n",
    "    # Further filter the DataFrame to remove adjectives with unwanted markers in their definitions,\n",
    "    # unless they are explicitly listed as allowed.\n",
    "    csv_df = csv_df[(~csv_df['Definition'].apply(lambda x: any(marker in x for marker in markers)) | csv_df['Adjective'].isin(allowed_words))]\n",
    "\n",
    "    # Sort the DataFrame by score (assuming a specific column naming convention) and trim to the desired length.\n",
    "    score_col = f\"{gender}_score\"  # This assumes a specific naming convention for score columns.\n",
    "    csv_df = csv_df.sort_values(by=score_col, ascending=False).head(min_length)\n",
    "\n",
    "    # Save the processed DataFrame back to both CSV and Parquet formats.\n",
    "    csv_df.to_csv(csv_file, index=False)\n",
    "    parquet_file = csv_file.replace('.csv', '.parquet')\n",
    "    csv_df.to_parquet(parquet_file, index=False)\n",
    "\n",
    "    print(f\"Updated DataFrame saved as {csv_file} and {parquet_file}\")\n",
    "\n",
    "    return csv_df\n",
    "\n",
    "def create_adjective_stimulus_files(csv_file):\n",
    "    \"\"\"\n",
    "    Copies an adjective list CSV file to a designated directory for stimulus files.\n",
    "\n",
    "    This function is designed to prepare stimulus files for experiments or further processing\n",
    "    by copying the specified CSV file containing an adjective list into a target directory.\n",
    "    It ensures that the target directory exists before copying and retains the original file name\n",
    "    in the target location.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the source CSV file containing the adjective list.\n",
    "    \"\"\"\n",
    "    # Define the directory where the stimulus files will be stored\n",
    "    target_dir = '../materials/adjectives/stimulus_files'\n",
    "    # Ensure the target directory exists, creating it if necessary\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the path for the target file within the stimulus directory\n",
    "    target_file_path = os.path.join(target_dir, os.path.basename(csv_file))\n",
    "\n",
    "    # Copy the CSV file to the target directory\n",
    "    shutil.copyfile(csv_file, target_file_path)\n",
    "    print(f\"Copied {csv_file} to {target_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(procedures, nouns_file, adjective_gender_association_method, top_n_adjectives, load_method, models, semantic_differential_vectors, use_groupby, remove_adjectives_with_markers, unallowed_words, allowed_words):\n",
    "    nouns_df = load_dataframe(nouns_file)\n",
    "    min_length = top_n_adjectives\n",
    "    if adjective_gender_association_method == 'cosine_similarity':\n",
    "        for gender in columns.keys(): columns[gender] = f'{gender}_similarity'\n",
    "    elif adjective_gender_association_method == 'semantic_differential':\n",
    "        for gender in columns.keys(): columns[gender] = f'{gender}_score'\n",
    "    if procedures['load_models']:\n",
    "        # Load models for each language\n",
    "        models = {lang: load_model(lang, load_method) for lang in languages}\n",
    "\n",
    "    if procedures['crawl_wiktionary_adjectives']:\n",
    "        # Adjectives extraction and saving\n",
    "        adjectives_url = f'https://en.wiktionary.org/wiki/Category:{lang_data[\"full_name\"]}_adjectives'\n",
    "        adjectives = extract_adjectives(lang_code, adjectives_url)\n",
    "        save_adjectives_to_parquet(adjectives, lang_code, f'adjectives/{lang_code}_adjectives.parquet')\n",
    "\n",
    "    if procedures['calculate_adjective_similarities']:\n",
    "        # Populate adjective list with gender similarity data\n",
    "        for lang_code in languages.keys():\n",
    "            print(f\"Performing gender similarity calculations for {languages[lang_code]['full_name']}...\")\n",
    "            calculate_adjective_similarities(lang_code)\n",
    "            print(f\"Calculations completed for {languages[lang_code]['full_name']}.\")\n",
    "\n",
    "    if procedures['select_top_adjectives']:\n",
    "        # Select the top n most masculine or feminine adjectives\n",
    "        for lang_code in languages.keys():\n",
    "            masculine, feminine = select_top_words(lang_code, num_rows=top_n_adjectives, method=adjective_gender_association_method, semantic_differential_vectors=semantic_differential_vectors)\n",
    "            if lang_code == 'es':\n",
    "                masculine, feminine = duplicate_spanish_adjectives(masculine, 'masculine'), duplicate_spanish_adjectives(feminine, 'feminine')\n",
    "            print(f\"Selected top adjectives for {languages[lang_code]['full_name']}: Masculine: {len(masculine)}, Feminine: {len(feminine)}\")\n",
    "    \n",
    "    if procedures['remove_adjective_duplicates']:\n",
    "        for lang_code in languages.keys():\n",
    "            remove_adjective_duplicates(lang_code, columns)\n",
    "        \n",
    "    # Turn Parquet files into csv files for manual inspection or readability during communication\n",
    "    if procedures['adjective_definition_review']:\n",
    "        for lang_code in languages.keys():\n",
    "            for gender in targets:\n",
    "                create_adjective_review_csv(f'../materials/adjectives/{lang_code}_{gender}_adjectives.parquet')\n",
    "\n",
    "    if procedures['remove_unwanted_adjectives']:\n",
    "        for lang_code in ['en', 'es', 'de']:\n",
    "            for gender in ['masculine', 'feminine']:\n",
    "                csv_file = f'../materials/adjectives/{lang_code}_{gender}_adjectives.csv'\n",
    "                parquet_file = f'../materials/adjectives/{lang_code}_{gender}_adjectives.parquet'\n",
    "                min_length = find_minimum_length(languages, targets, unallowed_words)\n",
    "                remove_unwanted_adjectives(csv_file, allowed_words, unallowed_words, remove_adjectives_with_markers, min_length, gender)\n",
    "\n",
    "    if procedures['create_stimulus_files']:\n",
    "        min_length = find_minimum_length(languages, targets, unallowed_words)\n",
    "\n",
    "        for lang_code in languages:\n",
    "            for gender in targets:\n",
    "                csv_file = f'../materials/adjectives/{lang_code}_{gender}_adjectives.csv'\n",
    "                create_adjective_stimulus_files(csv_file)\n",
    "\n",
    "            # Iterate through each language\n",
    "    for lang_code, lang_data in languages.items():\n",
    "        print(f\"Processing language: {lang_data['full_name']}\")\n",
    "        model = models[lang_code]\n",
    "\n",
    "        # Conduct Control Tests\n",
    "        if procedures['conduct_control_tests']:\n",
    "            print(f\"Conducting control tests for {lang_data['full_name']}\")\n",
    "            control_data_dir = f'../data/embeddings/control'\n",
    "            os.makedirs(control_data_dir, exist_ok=True)\n",
    "\n",
    "            control_data = create_control_test_dataframe(lang_code, nouns_df, model)\n",
    "            control_data.to_parquet(f'{control_data_dir}/{lang_code}_control_data.parquet')\n",
    "\n",
    "            for adj_association in targets:\n",
    "                control_data = create_control_test_dataframe(lang_code, nouns_df, model)\n",
    "                control_data.to_parquet(f'{control_data_dir}/{lang_code}_control_data.parquet')\n",
    "\n",
    "        # Conduct Experimental Tests\n",
    "        if procedures['conduct_experimental_tests']:\n",
    "            \n",
    "            print(f\"Conducting experimental tests for {lang_data['full_name']}\")\n",
    "            test_data_dir = f'test_data/{lang_code}'\n",
    "            os.makedirs(test_data_dir, exist_ok=True)\n",
    "\n",
    "            experimental_data = create_experimental_test_dataframe(lang_code, nouns_df, model, use_groupby)\n",
    "            experimental_data.to_parquet(f'{test_data_dir}/{lang_code}_test_data.parquet')\n",
    "\n",
    "    print(\"Finished processing for all languages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/rqv03bjn7rv2x6q_bv2828fh0000gn/T/ipykernel_43619/234502533.py:272: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  selected_masculine = pd.concat([selected_masculine, top_masculine])\n",
      "/var/folders/0b/rqv03bjn7rv2x6q_bv2828fh0000gn/T/ipykernel_43619/234502533.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  selected_feminine = pd.concat([selected_feminine, top_feminine])\n",
      "/var/folders/0b/rqv03bjn7rv2x6q_bv2828fh0000gn/T/ipykernel_43619/234502533.py:272: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  selected_masculine = pd.concat([selected_masculine, top_masculine])\n",
      "/var/folders/0b/rqv03bjn7rv2x6q_bv2828fh0000gn/T/ipykernel_43619/234502533.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  selected_feminine = pd.concat([selected_feminine, top_feminine])\n",
      "/var/folders/0b/rqv03bjn7rv2x6q_bv2828fh0000gn/T/ipykernel_43619/234502533.py:272: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  selected_masculine = pd.concat([selected_masculine, top_masculine])\n",
      "/var/folders/0b/rqv03bjn7rv2x6q_bv2828fh0000gn/T/ipykernel_43619/234502533.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  selected_feminine = pd.concat([selected_feminine, top_feminine])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top adjectives for English: Masculine: 100, Feminine: 100\n",
      "Selected top adjectives for Spanish: Masculine: 100, Feminine: 100\n",
      "Selected top adjectives for German: Masculine: 100, Feminine: 100\n",
      "Updated CSV files for en: removed duplicates with lower scores.\n",
      "Updated CSV files for es: removed duplicates with lower scores.\n",
      "Updated CSV files for de: removed duplicates with lower scores.\n",
      "Updated DataFrame saved as ../materials/adjectives/en_masculine_adjectives.csv and ../materials/adjectives/en_masculine_adjectives.parquet\n",
      "Updated DataFrame saved as ../materials/adjectives/en_feminine_adjectives.csv and ../materials/adjectives/en_feminine_adjectives.parquet\n",
      "Updated DataFrame saved as ../materials/adjectives/es_masculine_adjectives.csv and ../materials/adjectives/es_masculine_adjectives.parquet\n",
      "Updated DataFrame saved as ../materials/adjectives/es_feminine_adjectives.csv and ../materials/adjectives/es_feminine_adjectives.parquet\n",
      "Updated DataFrame saved as ../materials/adjectives/de_masculine_adjectives.csv and ../materials/adjectives/de_masculine_adjectives.parquet\n",
      "Updated DataFrame saved as ../materials/adjectives/de_feminine_adjectives.csv and ../materials/adjectives/de_feminine_adjectives.parquet\n",
      "Copied ../materials/adjectives/en_masculine_adjectives.csv to ../materials/adjectives/stimulus_files/en_masculine_adjectives.csv\n",
      "Copied ../materials/adjectives/en_feminine_adjectives.csv to ../materials/adjectives/stimulus_files/en_feminine_adjectives.csv\n",
      "Copied ../materials/adjectives/es_masculine_adjectives.csv to ../materials/adjectives/stimulus_files/es_masculine_adjectives.csv\n",
      "Copied ../materials/adjectives/es_feminine_adjectives.csv to ../materials/adjectives/stimulus_files/es_feminine_adjectives.csv\n",
      "Copied ../materials/adjectives/de_masculine_adjectives.csv to ../materials/adjectives/stimulus_files/de_masculine_adjectives.csv\n",
      "Copied ../materials/adjectives/de_feminine_adjectives.csv to ../materials/adjectives/stimulus_files/de_feminine_adjectives.csv\n",
      "Processing language: English\n",
      "Conducting control tests for English\n",
      "Conducting experimental tests for English\n",
      "Processing language: Spanish\n",
      "Conducting control tests for Spanish\n",
      "Conducting experimental tests for Spanish\n",
      "Processing language: German\n",
      "Conducting control tests for German\n",
      "Conducting experimental tests for German\n",
      "Finished processing for all languages.\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    procedures={\n",
    "        'load_models':False,\n",
    "        \n",
    "        'crawl_wiktionary_nouns':False,\n",
    "        'crawl_wiktionary_adjectives':False,\n",
    "\n",
    "        'calculate_adjective_similarities':False,\n",
    "\n",
    "        'select_top_adjectives':True,\n",
    "\n",
    "        'remove_adjective_duplicates':True,\n",
    "        'adjective_definition_review':False,\n",
    "        'remove_unwanted_adjectives':True,\n",
    "\n",
    "        'create_stimulus_files':True,\n",
    "\n",
    "        'conduct_control_tests':True,\n",
    "        'conduct_experimental_tests':True,\n",
    "    },\n",
    "    models=models,\n",
    "    load_method='normal',\n",
    "    # normal, facebook\n",
    "    nouns_file='../materials/nouns.csv',\n",
    "    top_n_adjectives=100,\n",
    "    adjective_gender_association_method='cosine_similarity',\n",
    "    use_groupby=True,\n",
    "    # for experimental tests, group adjectives for a given noun into one average cosine similarity, so instead of n(nouns)*n(adjectives) data points, you only have n(nouns) datapoints. best for strip plots to see individual points.\n",
    "    remove_adjectives_with_markers = [\"dated\", \"archaic\", \"dialectal\", \"rare\", \"ordinal number\", \"obsolete\", \"offensive\"],\n",
    "    # semantic differential\n",
    "    semantic_differential_vectors='gender1-gender2',\n",
    "    # gender1-gender2, gender-person\n",
    "    unallowed_words=['lesb', 'debonair', 'vestal', 'sunamita', 'negrid', 'Brummagem', 'follable', 'untervÃ¶gelt', 'schasaugert', 'Emeser', 'fÃ¼nfhundertste', 'Poppersch', 'SchlÃ¤nger', 'RÃ¶mer', 'Latina', 'titless', 'pussy', 'foine', 'mosuo', 'fÃ¡ustico', 'indio', 'rixig', 'hiborio', 'abgeschmack', 'kaki', 'klaviform', 'TK', 'antimalthusianisch', 'Danubian', 'eblaitisch', 'elfminÃ¼tig', 'Fregesch', 'jakobinisch', 'Malthusianisch', 'meiÃenisch', 'neunminÃ¼tig', 'rahn', 'vierzigminÃ¼tig', 'zwÃ¶lfminÃ¼tig', 'Afro-Latina', 'Dianic', 'Filipina', 'lady-like', 'MAAB', 'menstruate', 'obstetrical', 'Quebecoise', 'Rubenesque', 'woman-centric', 'vinny', 'twinky', 'Welshy', 'turrible', 'mick', 'fooking', 'particuler', 'legendry', 'awsome', 'roy', 'neo-Hegelian', 'phun', 'niiice', 'Democritean', 'Hegelian', 'Rothbardian', 'gent', 'afrodescendiente', 'axumita', 'curvi', 'delhita', 'feminazi', 'madrense', 'mizrajÃ­', 'oseta', 'postparto', 'sefaradita', 'sefardÃ­', 'sefardita', 'transgenerista', 'fuckin', 'hanbalitisch', 'antimalthusianisch', 'Malthusianisch', 'antimalthusianisch', 'malthusisch','gustiÃ¶s', 'hanbalitisch','handgehoben','scheiÃ', 'sturm', 'terrisch','Madonna-like','smoove', 'tuff','hench','insano', 'mofo', 'cutty', 'piff', 'jake', 'propa', 'mank','LGBT','papaya', 'child-bearing', 'plus-sized', 'post-partum', 'vulval', 'ben', 'unpossible', 'antifeminist', 'LGTB', 'LGTBI', 'babylonisch', 'erzgebirgisch', 'hinreissend', 'niedersorbisch', 'Sanct', 'sasanidisch', 'saudisch', 'altniederlÃ¤ndisch', 'bohrsch', 'britannisch', 'danubisch', 'dreiundvierzigminÃ¼tig', 'drittelzahlig', 'etatmÃ¤ssig', 'fÃ¼nfundzwanzigminÃ¼tig', 'fÃ¼nfunddreiÃigminÃ¼tig', 'fÃ¼nfminÃ¼tig', 'Hitlersch', 'koblenzisch', 'Luthersch', 'sechzigminÃ¼tig', 'sÃ¼datlantisch', 'Cesarean', 'prochoice', 'almight', 'cock-sure', 'cooool', 'nooby', 'peart', 'phantastic', 'Smithian', 'barakaldarra', 'bartorosellista', 'cefeida', 'chilota', 'dailamita', 'estambulita', 'kÃ¡bila', 'mazahua', 'ondarrutarra', 'ranjana', 'helle', 'zirkummediterran', 'sÃ¼datlantisch', 'preggers', 'vajazzled', 'shite', 'steezy', 'tinhorn', 'widdly', 'afrotropical', 'apollardado', 'mijita', 'ladilla', 'gray-haired', 'heavy-set', 'middleaged', 'Jew,' 'moustached', 'African-American', 'childbearing', 'Filipina', 'Madonna-like', 'newly-wed', 'Shunamite', 'Syrophoenician', 'teen-age', 'teen-aged', 'transgendered', 'grown-ass', 'mustached', 'Caucasian', 'biracial', 'mixed-race', 'thirties', 'forties', 'clean-shaved', 'moustachioed', 'dark-skinned', 'teenaged', 'mustachioed', 'ape', 'Afroestadounidense', 'Birracial', 'IndÃ­gena', 'sexi', 'Extraconyugal', 'Israelita', 'untrew', 'cristiano', 'jÃ³ven', 'afroestadounidense', 'birracial', 'Emesener', 'baktrisch', 'israelita', 'â¥-lich', 'vierzigmonatig', 'wÃ¤hrschaft', 'wolgadeutsch', 'amisch', 'dreiundfÃ¼nfzigjÃ¤hrig', 'kraftwerkisch', 'malisch', 'Palmyrer', 'Portaner' ,'achtundvierzigmonatig', 'padre', 'hypoÃ¤olisch', 'schwatt', 'sechsunddreiÃigmonatig', 'israelita', 'dreiÃigmonatig', 'einunddreiÃigeckig', 'schwul', 'mannmÃ¤nnlich'],\n",
    "    allowed_words=['rascal', 'transexual'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c717a47cdf72da10bd78b4a5153590ad14cef22d0e68537b3d97dcd083fccb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
