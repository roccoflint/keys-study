{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### completion model behavioral experiment\n",
    "#### NOTE: this behavioral experiment was not implemented in the CogSci 2024 conference submission.\n",
    "################################################################\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentence completion model\n",
    "model = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def make_api_call_for_model(phrase):\n",
    "    return model(phrase, max_length=len(phrase.split()) + 1, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "def get_probability_distribution(model_response):\n",
    "    words = model_response.split()\n",
    "    return [(words[-1], 1.0)]\n",
    "\n",
    "def sum_aggregated_probabilities(adjectives_list, apply_weighting=True):\n",
    "    if apply_weighting:\n",
    "        return sum((abs(4 - adjective[2]) ** 2) * adjective[1] for adjective in adjectives_list)\n",
    "    else:\n",
    "        return sum(adjective[1] for adjective in adjectives_list)\n",
    "\n",
    "def complete_phrase(phrase, masculine_adjectives, feminine_adjectives, apply_weighting=True):\n",
    "    predicted_gendered_adjectives = {'masculine': [], 'feminine': []}\n",
    "    for predicted_word in get_probability_distribution(make_api_call_for_model(phrase)):\n",
    "        for adjective_gender, adjectives in [('masculine', masculine_adjectives), ('feminine', feminine_adjectives)]:\n",
    "            for adjective in adjectives:\n",
    "                if adjective[0] == predicted_word[0]:\n",
    "                    predicted_gendered_adjectives[adjective_gender].append((adjective[0], predicted_word[1], adjective[1]))\n",
    "\n",
    "    data = {'masculine_aggregated_probabilities': 0, 'feminine_aggregated_probabilities': 0}\n",
    "    for gender in ['masculine', 'feminine']:\n",
    "        data[f'{gender}_aggregated_probabilities'] = sum_aggregated_probabilities(predicted_gendered_adjectives[gender], apply_weighting)\n",
    "    data['aggregated_probabilities_difference'] = data['masculine_aggregated_probabilities'] - data['feminine_aggregated_probabilities']\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "masculine_adjectives = [('strong', 5)]\n",
    "feminine_adjectives = [('beautiful', 5)]\n",
    "\n",
    "# Run with weighting enabled\n",
    "data_weighted = complete_phrase('The bridge is', masculine_adjectives, feminine_adjectives, True)\n",
    "print(data_weighted)\n",
    "\n",
    "# Run with weighting disabled\n",
    "data_unweighted = complete_phrase('The bridge is', masculine_adjectives, feminine_adjectives, False)\n",
    "print(data_unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### contextual word embeddings\n",
    "################################################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "print(\"Tokenizer and model loaded successfully.\")\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    # Flatten the vectors to ensure they're 1D.\n",
    "    v1_flat = v1.flatten()\n",
    "    v2_flat = v2.flatten()\n",
    "    # Calculate cosine similarity\n",
    "    cos_sim = np.dot(v1_flat, v2_flat) / (np.linalg.norm(v1_flat) * np.linalg.norm(v2_flat))\n",
    "    return cos_sim\n",
    "\n",
    "def get_word_embedding(sentence, word, layer='last', subword_aggregation='last'):\n",
    "    \"\"\"\n",
    "    Generate a contextual embedding for a specific word within a sentence,\n",
    "    with options to use the embedding from the first or last layer of the BERT model,\n",
    "    and to use the embedding of the first token, last token, or an aggregate of all the word's tokens.\n",
    "    \n",
    "    Args:\n",
    "    sentence (str): The sentence from which to extract the embedding.\n",
    "    word (str): The word to extract the embedding for.\n",
    "    layer (str): The layer of the BERT model to extract the embedding from - \"first\" or \"last\".\n",
    "    subword_aggregation (str): The method of aggregating subword token embeddings - \"first\", \"last\", or \"mean\".\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The requested word embedding.\n",
    "    \"\"\"\n",
    "    # Encode the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    if layer == 'first':\n",
    "        embeddings = outputs.hidden_states[0]\n",
    "    elif layer == 'last':\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    else:\n",
    "        raise ValueError(\"Invalid layer. Choose 'first' or 'last'.\")\n",
    "\n",
    "    # Tokenize the word and the sentence\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    sentence_tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # Find the indices of the first and last tokens of the word in the sentence\n",
    "    start_position = sentence_tokens.index(word_tokens[0])\n",
    "    end_position = start_position + len(word_tokens) - 1\n",
    "\n",
    "    # Extract the appropriate embedding based on the subword_aggregation method\n",
    "    if subword_aggregation == 'first':\n",
    "        word_embedding = embeddings[0, start_position]\n",
    "    elif subword_aggregation == 'last':\n",
    "        word_embedding = embeddings[0, end_position]\n",
    "    elif subword_aggregation == 'mean':\n",
    "        word_embedding = embeddings[0, start_position:end_position+1].mean(dim=0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid subword_aggregation. Choose 'first', 'last', or 'mean'.\")\n",
    "\n",
    "    return word_embedding.detach().numpy()\n",
    "\n",
    "def get_word_embedding_without_context(word, layer='last', subword_aggregation='last'):\n",
    "    \"\"\"\n",
    "    Generate a word embedding using BERT without any context,\n",
    "    with options to use the embedding from the first or last layer of the BERT model,\n",
    "    and to use the embedding of the first token, last token, or an aggregate of all the word's tokens.\n",
    "    \n",
    "    Args:\n",
    "    word (str): The word to extract the embedding for.\n",
    "    layer (str): The layer of the BERT model to extract the embedding from - \"first\" or \"last\".\n",
    "    subword_aggregation (str): The method of aggregating subword token embeddings - \"first\", \"last\", or \"mean\".\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The requested word embedding.\n",
    "    \"\"\"\n",
    "    # Encode the word\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    if layer == 'first':\n",
    "        embeddings = outputs.hidden_states[0]\n",
    "    elif layer == 'last':\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    else:\n",
    "        raise ValueError(\"Invalid layer. Choose 'first' or 'last'.\")\n",
    "\n",
    "    if subword_aggregation == 'first':\n",
    "        word_embedding = embeddings[0, 1]  # Use the embedding of the first token (excluding [CLS])\n",
    "    elif subword_aggregation == 'last':\n",
    "        word_embedding = embeddings[0, -2]  # Use the embedding of the last token (excluding [SEP])\n",
    "    elif subword_aggregation == 'mean':\n",
    "        word_embedding = embeddings[0, 1:-1].mean(dim=0)  # Use the mean of all token embeddings (excluding [CLS] and [SEP])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid subword_aggregation. Choose 'first', 'last', or 'mean'.\")\n",
    "\n",
    "    return word_embedding.detach().numpy()\n",
    "\n",
    "def calculate_similarity(reference, target, language, is_control, grammatical_gender=None, layer='last', subword_aggregation='last', use_context=True):\n",
    "    if use_context:\n",
    "        if grammatical_gender:\n",
    "            if language == 'es':\n",
    "                if grammatical_gender == 'feminine': \n",
    "                    reference_sentence = f\"la {reference} es\"\n",
    "                    target_sentence = f\"la {reference} es {target}\"  \n",
    "                else: \n",
    "                    reference_sentence = f\"el {reference} es\"\n",
    "                    target_sentence = f\"el {reference} es {target}\"\n",
    "            elif language == 'de':\n",
    "                if grammatical_gender == 'feminine':\n",
    "                    reference_sentence = f\"die {reference} ist\"\n",
    "                    target_sentence = f\"die {reference} ist {target}\"\n",
    "                elif grammatical_gender == 'masculine':\n",
    "                    reference_sentence = f\"der {reference} ist\"\n",
    "                    target_sentence = f\"der {reference} ist {target}\"\n",
    "                else:\n",
    "                    reference_sentence = f\"das {reference} ist\"\n",
    "                    target_sentence = f\"das {reference} ist {target}\"\n",
    "            else:\n",
    "                reference_sentence = f\"the {reference} is\"\n",
    "                target_sentence = f\"the {reference} is {target}\"\n",
    "            \n",
    "            reference_embedding = get_word_embedding(reference_sentence, reference, layer, subword_aggregation)\n",
    "            target_embedding = get_word_embedding(target_sentence, target, layer, subword_aggregation)\n",
    "        else:\n",
    "            reference_sentence = f\"{reference}\"\n",
    "            target_sentence = f\"{target}\"\n",
    "            reference_embedding = get_word_embedding(reference_sentence, reference, layer, subword_aggregation)\n",
    "            target_embedding = get_word_embedding(target_sentence, target, layer, subword_aggregation)\n",
    "    else:\n",
    "        reference_embedding = get_word_embedding_without_context(reference, layer, subword_aggregation)\n",
    "        target_embedding = get_word_embedding_without_context(target, layer, subword_aggregation)\n",
    "    \n",
    "    similarity = cosine_similarity(reference_embedding, target_embedding)\n",
    "    experiment = \"CNTRL\" if is_control else \"EXPRMNTL\" \n",
    "    print(f\"[{experiment}] Similarity calculated for {reference} and {target} in {language}\")\n",
    "    return similarity\n",
    "\n",
    "def process_language(language, is_control=False, layer='last', subword_aggregation='last', use_context=True):\n",
    "    print(f\"Processing {'control' if is_control else 'experimental'} data for language: {language}\")\n",
    "    \n",
    "    if is_control:\n",
    "        input_file = f\"../data/embeddings/control/{language}_embeddings_control.csv\"\n",
    "        output_file = f\"../data/contextual-embeddings/control/{language}_contextual-embeddings_control_layer-{layer}_subword-{subword_aggregation}_context-{use_context}.csv\"\n",
    "    else:\n",
    "        input_file = f\"../data/embeddings/experimental/{language}_embeddings_experimental.csv\"\n",
    "        output_file = f\"../data/contextual-embeddings/experimental/{language}_contextual-embeddings_experimental_layer-{layer}_subword-{subword_aggregation}_context-{use_context}.csv\"\n",
    "    \n",
    "    print(f\"Input file: {input_file}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    print(\"Reading input CSV file...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Input CSV file read successfully. Shape: {df.shape}\")\n",
    "    \n",
    "    # Calculate contextual similarity for each row and replace the values in the 'COSINE SIMILARITY' column\n",
    "    print(\"Calculating contextual similarity for each row...\")\n",
    "    if is_control:\n",
    "        df['COSINE SIMILARITY'] = df.apply(lambda row: calculate_similarity(row['REFERENCE WORD'], row['TARGET WORD'], language, is_control, layer=layer, subword_aggregation=subword_aggregation, use_context=use_context), axis=1)\n",
    "    else:\n",
    "        df['COSINE SIMILARITY'] = df.apply(lambda row: calculate_similarity(row['NOUN'], row['ADJECTIVE'], language, is_control, row['GRAMMATICAL GENDER OF NOUN'], layer=layer, subword_aggregation=subword_aggregation, use_context=use_context), axis=1)\n",
    "    print(\"Contextual similarity calculation completed.\")\n",
    "    \n",
    "    # Save the updated DataFrame to the output CSV file\n",
    "    print(f\"Saving updated DataFrame to output CSV file: {output_file}\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Output CSV file saved successfully.\")\n",
    "    \n",
    "    print(f\"Processing completed for {'control' if is_control else 'experimental'} data for language: {language}\\n\")\n",
    "\n",
    "languages = ['en', 'es', 'de']\n",
    "layer_options = ['first', 'last']\n",
    "subword_aggregation_options = ['first', 'last', 'mean']\n",
    "use_context_options = [True, False]\n",
    "\n",
    "for language in languages:\n",
    "    for layer in layer_options:\n",
    "        for subword_aggregation in subword_aggregation_options:\n",
    "            for use_context in use_context_options:\n",
    "                process_language(language, is_control=True, layer=layer, subword_aggregation=subword_aggregation, use_context=use_context)\n",
    "                process_language(language, is_control=False, layer=layer, subword_aggregation=subword_aggregation, use_context=use_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0 (main, Nov  1 2023, 16:36:05) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c717a47cdf72da10bd78b4a5153590ad14cef22d0e68537b3d97dcd083fccb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
